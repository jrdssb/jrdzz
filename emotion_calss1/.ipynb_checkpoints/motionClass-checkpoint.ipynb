{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d72430-7f04-43c2-8eca-d5a7fa9acadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "# 进行csv的生成\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "# from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "#从本地文件提取文件路径(要用/)，标签，保存至info_array数组中，到处为motion.csv文件\n",
    "dataset_dir='./train'\n",
    "dataset_posdir='./train/pos'\n",
    "dataset_negdir='./train/neg'\n",
    "classes=os.listdir(dataset_posdir)\n",
    "\n",
    "info_array=[]\n",
    "col=['index','score','label','filepath']\n",
    "# 设置需要的数据\n",
    "for filename in os.listdir(dataset_posdir):\n",
    "    filepath=dataset_posdir+'/'+filename\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    split_name=filename_without_extension.split('_')\n",
    "    index=split_name[0]\n",
    "    score=split_name[1]\n",
    "    # label='pos'\n",
    "    label=1\n",
    "    info_array.append([index,score,label,filepath])\n",
    "for filename in os.listdir(dataset_negdir):\n",
    "    filepath=dataset_negdir+'/'+filename\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    split_name=filename_without_extension.split('_')\n",
    "    index=split_name[0]\n",
    "    score=split_name[1]\n",
    "    # label='neg'\n",
    "    label=0\n",
    "    info_array.append([index,score,label,filepath])\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "numpy_info_array = np.array(info_array)  \n",
    "df=pd.DataFrame(numpy_info_array,columns=col)\n",
    "df.to_csv('./motion.csv',encoding='utf-8')\n",
    "\n",
    "# 重写dataset数据集部分，定义数据格式\n",
    "class motionDataset(Dataset):\n",
    "    def __init__(self,dataset_dir,csv_path):\n",
    "        self.csv_path=csv_path\n",
    "        self.df=pd.read_csv(self.csv_path,encoding='utf-8')\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        # 获取目标文本\n",
    "        # memo_content储存一个文件中的文本，保存数据类型是字符串，做了去除除单词符号外元素的处理\n",
    "        if idx >= len(self.df):\n",
    "            raise IndexError(\"Index out of range.\")\n",
    "        memo_filepath=self.df['filepath'][idx]\n",
    "        with open(memo_filepath,'r',encoding='utf-8')as f:\n",
    "            memo_content=f.read()\n",
    "        memo_content=re.sub(r'[^\\w\\s]', '', memo_content)  \n",
    "        \n",
    "        encoding = tokenizer.encode_plus(\n",
    "            memo_content,\n",
    "            add_special_tokens=True,\n",
    "            max_length=500,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        token_type_ids = encoding['token_type_ids']\n",
    "\n",
    "        # 将类别保存为y_train，数据类型为字符串\n",
    "        y_train = self.df['label'][idx]\n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids, 'label': y_train}\n",
    "train_ds=motionDataset('./train','./motion.csv')\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ce4338-0723-485d-9130-24e0895ab815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 5000\n"
     ]
    }
   ],
   "source": [
    "# 对训练集做分割处理，便于后续训练测试   random_split\n",
    "from torch.utils.data import random_split\n",
    "num_sample=len(train_ds)\n",
    "train_percent=0.8\n",
    "train_num=int(train_percent*num_sample)\n",
    "test_num=num_sample-train_num\n",
    "train_ds1,train_ds2=random_split(train_ds,[train_num,test_num])\n",
    "print(len(train_ds1),len(train_ds2))\n",
    "\n",
    "# 批处理函数，由于各个文本长不同，分别使用0进行填充，同时label转换成tensor类型数据\n",
    "def collate_fn(data):\n",
    "    # 按input_ids长度进行降序排序，提高填充效率\n",
    "    data.sort(key=lambda x: len(x['input_ids']), reverse=True)\n",
    "\n",
    "    max_length = 500  # 设置一个最大长度，根据模型的最大长度进行调整，同时把数据处理成模型需要数据的形状\n",
    "\n",
    "    input_ids = [item['input_ids'][:, :max_length] for item in data]  # 截断或填充到相同的长度\n",
    "    attention_mask = [item['attention_mask'][:, :max_length] for item in data]\n",
    "    token_type_ids = [item['token_type_ids'][:, :max_length] for item in data] \n",
    "    labels = [item['label'] for item in data]\n",
    "\n",
    "    # Pad sequences after sorting   batch_first表示张量输出维度是第一个维度   padding_value表示用0填充     squeeze表示去除不需要的维度\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0).squeeze(1)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0).squeeze(1)\n",
    "    padded_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0).squeeze(1)\n",
    "\n",
    "    # Convert labels to one-dimensional tensor，后续模型需要long类型的数据，将label转化为long的tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # return {'input_ids': padded_input_ids, 'attention_mask': padded_attention_mask,\n",
    "    #         'token_type_ids': padded_token_type_ids, 'label': labels}   要直接返回新数据名称\n",
    "    return padded_input_ids, padded_attention_mask, padded_token_type_ids, labels      \n",
    "    \n",
    "# 重写dataloader，定义批次大小，collate_fn函数，打乱\n",
    "train1_dataloader=DataLoader(train_ds1,batch_size=16,shuffle=True,collate_fn=collate_fn)\n",
    "train2_dataloader=DataLoader(train_ds2,batch_size=16,shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "# def collate_fn(data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857d4dea-eae3-4307-923c-efadbd2ba5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载字典和分词工具\n",
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型，迁移学习，使用一个在大规模文本数据上预训练的模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-uncased').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5f7e48-0e35-4625-bbf7-2e8e00468f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,pretrained):\n",
    "        # 这里要把pretrained嵌入\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "        # fc:输出层，第一个参数为全连接层的输入，第二个参数时表明输出为两个分类，该函数让模型\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            # 用训练模型做计算，将抽取的特征放到全连接神经网络中\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        # 取零和bert设计有关\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model(pretrained)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "# 遍历所有model中的所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c48401d-c203-455b-a035-77d1c9871b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.83315 0.030180568824708463\n",
      "2\n",
      "0.83975 0.02967965843230486\n",
      "3\n",
      "0.8437 0.029346155707538128\n",
      "4\n",
      "0.84675 0.029126595240831374\n",
      "5\n",
      "0.84855 0.02898122247606516\n",
      "6\n",
      "0.85075 0.028824644580483436\n",
      "7\n",
      "0.85055 0.02871748948097229\n",
      "8\n",
      "0.85335 0.028625473208725452\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=0.0003)\n",
    "# 交叉熵计算损失，做梯度下降处理\n",
    "criterion = torch.nn.CrossEntropyLoss().to(\"cuda:0\")\n",
    "model.train()\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch+1)\n",
    "    correct=0\n",
    "    totalLoss=0\n",
    "    total=0\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train1_dataloader):\n",
    "        input_ids=input_ids.to(\"cuda:0\")   \n",
    "        attention_mask=attention_mask.to(\"cuda:0\")   \n",
    "        token_type_ids=token_type_ids.to(\"cuda:0\")   \n",
    "        labels=labels.to(\"cuda:0\")\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)   \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        out = out.argmax(dim=1) \n",
    "        total+=len(labels)\n",
    "        correct+=(out == labels).sum().item()\n",
    "        totalLoss+=loss.item()\n",
    "        # if i % 10 == 0:\n",
    "        #     out = out.argmax(dim=1) \n",
    "        #     # 获取out输出张量中沿第一个维度找到每行中的最大值的索引\n",
    "        #     accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        # if i == 1200:\n",
    "        #     print(loss.item(), accuracy)\n",
    "    print(correct/total,totalLoss/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f91b532-0781-458a-a003-873dbb1eb6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir1='./test'\n",
    "dataset_posdir1='./train/pos'\n",
    "dataset_negdir1='./train/neg'\n",
    "classes=os.listdir(dataset_posdir1)\n",
    "\n",
    "info_array1=[]\n",
    "col1=['index','score','label','filepath']\n",
    "\n",
    "for filename in os.listdir(dataset_posdir1):\n",
    "    filepath=dataset_posdir1+'/'+filename\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    split_name=filename_without_extension.split('_')\n",
    "    index=split_name[0]\n",
    "    score=split_name[1]\n",
    "    # label='pos'\n",
    "    label=1\n",
    "    info_array1.append([index,score,label,filepath])\n",
    "for filename in os.listdir(dataset_negdir1):\n",
    "    filepath=dataset_negdir1+'/'+filename\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    split_name=filename_without_extension.split('_')\n",
    "    index=split_name[0]\n",
    "    score=split_name[1]\n",
    "    # label='neg'\n",
    "    label=0\n",
    "    info_array1.append([index,score,label,filepath])\n",
    "\n",
    "numpy_info_array1 = np.array(info_array1)  \n",
    "df=pd.DataFrame(numpy_info_array1,columns=col1)\n",
    "df.to_csv('./emotion_test.csv',encoding='utf-8')\n",
    "\n",
    "test_ds=motionDataset('./test','./emotion_test.csv')\n",
    "\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab5dbac-1530-4acb-be2c-f7fb3ab5d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21361 25000 0.85444\n"
     ]
    }
   ],
   "source": [
    "test_dataloader=DataLoader(test_ds,batch_size=16,shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(test_dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids=input_ids.to(\"cuda:0\")   \n",
    "            attention_mask=attention_mask.to(\"cuda:0\")   \n",
    "            token_type_ids=token_type_ids.to(\"cuda:0\")   \n",
    "            labels=labels.to(\"cuda:0\")\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct,total,correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef7601-174a-4ff2-a6f6-c4ba923d4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list=[]\n",
    "pred_list.append(out.tolist()[0])\n",
    "# 获取输出的概率向量\n",
    "labels_name=[\"positive\",\"negative\"]\n",
    "df_pred=pd.DataFrame(data=pred,columns=label_name)\n",
    "df_pred.to_csv('pred_result.csv',encoding='gbk',index=false)\n",
    "print(\"Done!\")\n",
    "\n",
    "# 绘制roc曲线\n",
    "from sklearn.metrics import * \n",
    "# pip install scikit-learn\n",
    "import matplotlib.pyplot as plt \n",
    "# pip install matplotlib\n",
    "import numpy as np\n",
    "# pip install numpy\n",
    "from sklearn.preprocessing import binarize\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据，需要读取模型输出的标签和正确的标签\n",
    "target_data=pd.read_csv(文件路径，sep=\"\\t\" 分隔符 .names=[])\n",
    "true_label\n",
    "\n",
    "# 预测的标签\n",
    "predict_label=predict_data.to_numpy().argmax(axis=1)\n",
    "\n",
    "# 常用指标的计算：精度，查准率，召回率，f1-score\n",
    "# 1.精度。预测正确的数量占所有样本比例\n",
    "accuracy=accuracy_score(true_label,predict_label)\n",
    "\n",
    "# 2.查准率 TP/(TP+FP),所有判断为正的样本中判断正确的比例\n",
    "precision=precision_score(true_label,predict_label,labels=None,pos_label=1,average='micro')\n",
    "# micro是多类别融合的方法\n",
    "\n",
    "# 3.召回率 TP/(TP+FN)判断正确的正样本占所有正样本的比例（TPR）\n",
    "recall=recall_score(true_label,predict_label,average='micro')\n",
    "\n",
    "# 4.f1-score\n",
    "f1=f1_score(true_label,predict_label,average='micro')\n",
    "\n",
    "# 混淆矩阵\n",
    "# 类别标签名\n",
    "label_names={}\n",
    "\n",
    "# 多分类roc曲线\n",
    "n_calsses=len(labels_names)\n",
    "binarize_predict=label_binarize(predict_label,classes=[i for i in range(n_classes)]\n",
    "# 将预测标签转化为二进制格式便于后续roc曲线的计算\n",
    "\n",
    "# 读取预测结果，转化为numpy数组\n",
    "predict_score = predict_data.to_numpy()\n",
    "\n",
    "# 计算每一类的ROC\n",
    "\n",
    "# 创建了三个字典对象，对于后续每个类别i，fpr[i]将存储该类别的假阳率，真阳率，auc值\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # 对每一个类别计算roc曲线   roc_curve计算每个类别的TPR，FPR\n",
    "    # [:,i]binarize_predict是一个二维数组，取这个二维数组的每一列，就是取每个样本对应于第i类分类的分类情况\n",
    "    # [socre_i[i] for socre_i in predict_score]是列表解析式，构建了一个新的列表，它包含了所有样本在第i分类的得分\n",
    "    fpr[i], tpr[i], _ = roc_curve(binarize_predict[:,i], [socre_i[i] for socre_i in predict_score])\n",
    "    # 计算曲线下面积\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# print(\"roc_auc = \",roc_auc)\n",
    "\n",
    "# 得到所有类别假阳率的唯一值：ROC曲线是基于不同的假阳率来绘制真阳率的，如果假阳率存在重复值，那么在绘制曲线时可能会出现同一个假阳率对应多个真阳率的情况\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "# Plot all ROC curves\n",
    "\n",
    "# line width设置曲线宽度\n",
    "lw = 2\n",
    "\n",
    "# 创建一个新的图像\n",
    "plt.figure()\n",
    "\n",
    "# 绘制宏平均 ROC 曲线以及每个类别的 ROC 曲线\n",
    "# 宏观平均（macro-average），计算所有真阳率和假阳率的平均值得到总体性能指标\n",
    "# label语句参数，用来设置图例，显示的文本信息{0:0.2f} 是一个格式化字符串，用于将 roc_auc[\"macro\"] 的值插入到字符串中，并保留两位小数\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=lw, label='ROC curve of {0} (area = {1:0.2f})'.format(label_names[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "\n",
    "# 设置xy轴的范围\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# 设置xy轴的标签\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# 设置标题\n",
    "plt.title('Multi-class receiver operating characteristic ')\n",
    "\n",
    "# 增加图例\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998297af-c29f-4c09-bd96-6454256d3505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2574b793-3d0c-49f6-8bb2-34749cb1b6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# 可以使用.pop除去不需要的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be029d25-7e1e-4505-abe1-1fc183efbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#附1，dataset部分\n",
    "        \n",
    "        # tokens=tokenizer.tokenize(memo_content)  可以分割文本，做分词处理：eg：unpretraining=>un+##pretrain+##ing\n",
    "        # tokens=['CLS']+tokens+['SEP']\n",
    "\n",
    "        # 文本长度处理1：\n",
    "        \n",
    "        # # 设置目标向量的长度\n",
    "        # target_length = 500\n",
    "        # # 如果当前向量长度不足，使用 ['PAD'] 进行填充\n",
    "        # if current_length < target_length:\n",
    "        #     padding_tokens = ['[PAD]'] * (target_length - current_length)\n",
    "        #     tokens += padding_tokens\n",
    "        # token_ids = tokenizer.convert_tokens_to_ids(tokens)   tokenizer.convert_tokens_to ids将单词映射成不同id\n",
    "        # token_ids = torch.tensor(token_ids).unsqueeze(0)     降维，unsqueeze把获得的token_ids转化为需要的形状\n",
    "        # attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]    把文本部分设为1，空白部分设为0\n",
    "        # attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "        # 把tokens_id和attention_mask转化为tensor向量\n",
    "        # # 将该数据保存为x_tarin\n",
    "        # y_train=self.df['label'][idx]\n",
    "        # 将类别保存为y_train，数据类型为字符串\n",
    "        # return tokens,token_ids,attention_mask,y_train\n",
    "        # return {'token_ids':token_ids,'attention_mask':attention_mask,'label':y_train  以字典序格式返回需要的数据\n",
    "        \n",
    "        # 分词与编码：使用BertTokenizer的encode_plus方法获取token_ids、attention_mask和token_type_ids\n",
    "\n",
    "# 附2 调试部分，打印获取一个批次的attention_mask.token_ids,token_type_ids    将collate_fn返回值修改之后报错？\n",
    "# for i, batch in enumerate(train_dataloader):           \n",
    "#     input_ids = batch['input_ids']\n",
    "#     attention_mask = batch['attention_mask']\n",
    "#     token_type_ids = batch['token_type_ids']\n",
    "#     y_train = batch['label']\n",
    "\n",
    "#     print(input_ids)\n",
    "#     print(attention_mask)\n",
    "#     print(token_type_ids)\n",
    "#     print(y_train)\n",
    "#     break\n",
    "# print(len(train_dataloader))\n",
    "# input_ids.shape, attention_mask.shape,token_type_ids.shape,y_train.shape\n",
    "\n",
    "# 附3  数据格式转化\n",
    "# 1.\n",
    "# attention_mask=[1 if i!='[PAD]' else 0 for i in tokens]\n",
    "# attention_mask\n",
    "# 2.\n",
    "# tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "# tokens_ids\n",
    "# 3.\n",
    "# numpy_info_array = np.array(info_array)  \n",
    "# print(numpy_info_array.shape)\n",
    "# 4.\n",
    "# df=pd.DataFrame(numpy_info_array,columns=col)\n",
    "# df.to_csv('./motion.csv',encoding='utf-8')\n",
    "# 5.\n",
    "# tokens_ids=torch.tensor(tokens_ids).unsqueeze(0)\n",
    "# attention_mask=torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "# 附4  使用transformers从所有的编码器层中抽取嵌入表示。\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# sentence = 'I love Paris'\n",
    "# tokens = tokenizer.tokenize(sentence)\n",
    "# tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "# tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "# attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "# token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "# attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "# last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask = attention_mask, return_dict = False)\n",
    "# 参数：last_hidden_state包含所有标记的嵌入表示，但是仅来自最后一个编码器层(encoder 12)\n",
    "#       pooler_output代表从最后的编码器层得到的[CLS]标记对应的嵌入表示，但进一步地通过一个线性和tanh激活函数(BertPooler)处理。\n",
    "#       hidden_states包含从所有编码器层得到的所有标记的嵌入表示\n",
    "# 1.调用模型2.切分句子3.加前后缀4.提取attention_mask5.每个标记映射到对应的id6.转化成tensor\n",
    "# class BertPooler(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "#         # to the first token.\n",
    "#         first_token_tensor = hidden_states[:, 0]\n",
    "#         # 线性\n",
    "#         pooled_output = self.dense(first_token_tensor)\n",
    "#         # tanh\n",
    "#         pooled_output = self.activation(pooled_output)\n",
    "#         return pooled_output\n",
    "# last_hidden_state.shape\n",
    "# # 它包含最后的编码器层得到的[CLS]标记对应的嵌入表示。我们打印它的形状：\n",
    "# pooler_output.shape\n",
    "# len(hidden_states)\n",
    "# hidden_states。它是一个包含13个值的元组，保存了从输入层h0到最后一个编码器层h12的所有嵌入表示：\n",
    "# 这样我们就可以得到所有编码器层的标记对应的嵌入表示\n",
    "# hidden_states[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
