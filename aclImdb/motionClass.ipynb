{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d72430-7f04-43c2-8eca-d5a7fa9acadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "# 进行csv的生成\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "# from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "#从本地文件提取文件路径(要用/)，标签，保存至info_array数组中，到处为motion.csv文件\n",
    "dataset_dir='./train'\n",
    "dataset_posdir='./train/pos'\n",
    "dataset_negdir='./train/neg'\n",
    "classes=os.listdir(dataset_posdir)\n",
    "\n",
    "info_array=[]\n",
    "col=['index','score','label','filepath']\n",
    "# 设置需要的数据\n",
    "for filename in os.listdir(dataset_posdir):\n",
    "    filepath=dataset_posdir+'/'+filename\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    split_name=filename_without_extension.split('_')\n",
    "    index=split_name[0]\n",
    "    score=split_name[1]\n",
    "    # label='pos'\n",
    "    label=1\n",
    "    info_array.append([index,score,label,filepath])\n",
    "for filename in os.listdir(dataset_negdir):\n",
    "    filepath=dataset_negdir+'/'+filename\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    split_name=filename_without_extension.split('_')\n",
    "    index=split_name[0]\n",
    "    score=split_name[1]\n",
    "    # label='neg'\n",
    "    label=0\n",
    "    info_array.append([index,score,label,filepath])\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "numpy_info_array = np.array(info_array)  \n",
    "df=pd.DataFrame(numpy_info_array,columns=col)\n",
    "df.to_csv('./motion.csv',encoding='utf-8')\n",
    "\n",
    "# 重写dataset数据集部分，定义数据格式\n",
    "class motionDataset(Dataset):\n",
    "    def __init__(self,dataset_dir,csv_path):\n",
    "        self.csv_path=csv_path\n",
    "        self.df=pd.read_csv(self.csv_path,encoding='utf-8')\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        # 获取目标文本\n",
    "        # memo_content储存一个文件中的文本，保存数据类型是字符串，做了去除除单词符号外元素的处理\n",
    "        if idx >= len(self.df):\n",
    "            raise IndexError(\"Index out of range.\")\n",
    "        memo_filepath=self.df['filepath'][idx]\n",
    "        with open(memo_filepath,'r',encoding='utf-8')as f:\n",
    "            memo_content=f.read()\n",
    "        memo_content=re.sub(r'[^\\w\\s]', '', memo_content)  \n",
    "        \n",
    "        encoding = tokenizer.encode_plus(\n",
    "            memo_content,\n",
    "            add_special_tokens=True,\n",
    "            max_length=500,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        token_type_ids = encoding['token_type_ids']\n",
    "\n",
    "        # 将类别保存为y_train，数据类型为字符串\n",
    "        y_train = self.df['label'][idx]\n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids, 'label': y_train}\n",
    "train_ds=motionDataset('./train','./motion.csv')\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ce4338-0723-485d-9130-24e0895ab815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 5000\n"
     ]
    }
   ],
   "source": [
    "# 对训练集做分割处理，便于后续训练测试   random_split\n",
    "from torch.utils.data import random_split\n",
    "num_sample=len(train_ds)\n",
    "train_percent=0.8\n",
    "train_num=int(train_percent*num_sample)\n",
    "test_num=num_sample-train_num\n",
    "train_ds1,train_ds2=random_split(train_ds,[train_num,test_num])\n",
    "print(len(train_ds1),len(train_ds2))\n",
    "\n",
    "# 批处理函数，由于各个文本长不同，分别使用0进行填充，同时label转换成tensor类型数据\n",
    "def collate_fn(data):\n",
    "    # 按input_ids长度进行降序排序，提高填充效率\n",
    "    data.sort(key=lambda x: len(x['input_ids']), reverse=True)\n",
    "\n",
    "    max_length = 500  # 设置一个最大长度，根据模型的最大长度进行调整，同时把数据处理成模型需要数据的形状\n",
    "\n",
    "    input_ids = [item['input_ids'][:, :max_length] for item in data]  # 截断或填充到相同的长度\n",
    "    attention_mask = [item['attention_mask'][:, :max_length] for item in data]\n",
    "    token_type_ids = [item['token_type_ids'][:, :max_length] for item in data] \n",
    "    labels = [item['label'] for item in data]\n",
    "\n",
    "    # Pad sequences after sorting   batch_first表示张量输出维度是第一个维度   padding_value表示用0填充     squeeze表示去除不需要的维度\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0).squeeze(1)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0).squeeze(1)\n",
    "    padded_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0).squeeze(1)\n",
    "\n",
    "    # Convert labels to one-dimensional tensor，后续模型需要long类型的数据，将label转化为long的tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # return {'input_ids': padded_input_ids, 'attention_mask': padded_attention_mask,\n",
    "    #         'token_type_ids': padded_token_type_ids, 'label': labels}   要直接返回新数据名称\n",
    "    return padded_input_ids, padded_attention_mask, padded_token_type_ids, labels      \n",
    "    \n",
    "# 重写dataloader，定义批次大小，collate_fn函数，打乱\n",
    "train1_dataloader=DataLoader(train_ds1,batch_size=64,shuffle=True,collate_fn=collate_fn)\n",
    "train2_dataloader=DataLoader(train_ds2,batch_size=64,shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "# def collate_fn(data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857d4dea-eae3-4307-923c-efadbd2ba5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载字典和分词工具\n",
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型，迁移学习，使用一个在大规模文本数据上预训练的模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-uncased').to(\"cuda:0\")\n",
    "\n",
    "#不训练,不需要计算梯度：使用fun_tuning,冻结预训练模型参数，只训练下游任务模型\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5f7e48-0e35-4625-bbf7-2e8e00468f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,pretrained):\n",
    "        # 这里要把pretrained嵌入\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "        # 全连接神经网络，单层网络模型，二分类\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "            # 用训练模型做计算，将抽取的特征放到全连接神经网络中\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        # 取零和bert设计有关\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model(pretrained)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")\n",
    "# 模型实例化\n",
    "for param in model.parameters():\n",
    "    print(param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48401d-c203-455b-a035-77d1c9871b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 0.6149067878723145 0.734375\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=0.00005)\n",
    "# 交叉熵计算损失，做梯度下降处理\n",
    "criterion = torch.nn.CrossEntropyLoss().to(\"cuda:0\")\n",
    "model.train()\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train1_dataloader):\n",
    "        input_ids=input_ids.to(\"cuda:0\")   \n",
    "        attention_mask=attention_mask.to(\"cuda:0\")   \n",
    "        token_type_ids=token_type_ids.to(\"cuda:0\")   \n",
    "        labels=labels.to(\"cuda:0\")\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)   \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        if i == 312:\n",
    "            print(i, loss.item(), accuracy)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f91b532-0781-458a-a003-873dbb1eb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader=DataLoader(test_ds,batch_size=250,shuffle=True,collate_fn=collate_fn)\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(test_dataloader):\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be029d25-7e1e-4505-abe1-1fc183efbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#附1，dataset部分\n",
    "        \n",
    "        # tokens=tokenizer.tokenize(memo_content)  可以分割文本，做分词处理：eg：unpretraining=>un+##pretrain+##ing\n",
    "        # tokens=['CLS']+tokens+['SEP']\n",
    "\n",
    "        # 文本长度处理1：\n",
    "        \n",
    "        # # 设置目标向量的长度\n",
    "        # target_length = 500\n",
    "        # # 如果当前向量长度不足，使用 ['PAD'] 进行填充\n",
    "        # if current_length < target_length:\n",
    "        #     padding_tokens = ['[PAD]'] * (target_length - current_length)\n",
    "        #     tokens += padding_tokens\n",
    "        # token_ids = tokenizer.convert_tokens_to_ids(tokens)   tokenizer.convert_tokens_to ids将单词映射成不同id\n",
    "        # token_ids = torch.tensor(token_ids).unsqueeze(0)     降维，unsqueeze把获得的token_ids转化为需要的形状\n",
    "        # attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]    把文本部分设为1，空白部分设为0\n",
    "        # attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "        # 把tokens_id和attention_mask转化为tensor向量\n",
    "        # # 将该数据保存为x_tarin\n",
    "        # y_train=self.df['label'][idx]\n",
    "        # 将类别保存为y_train，数据类型为字符串\n",
    "        # return tokens,token_ids,attention_mask,y_train\n",
    "        # return {'token_ids':token_ids,'attention_mask':attention_mask,'label':y_train  以字典序格式返回需要的数据\n",
    "        \n",
    "        # 分词与编码：使用BertTokenizer的encode_plus方法获取token_ids、attention_mask和token_type_ids\n",
    "\n",
    "# 附2 调试部分，打印获取一个批次的attention_mask.token_ids,token_type_ids    将collate_fn返回值修改之后报错？\n",
    "# for i, batch in enumerate(train_dataloader):           \n",
    "#     input_ids = batch['input_ids']\n",
    "#     attention_mask = batch['attention_mask']\n",
    "#     token_type_ids = batch['token_type_ids']\n",
    "#     y_train = batch['label']\n",
    "\n",
    "#     print(input_ids)\n",
    "#     print(attention_mask)\n",
    "#     print(token_type_ids)\n",
    "#     print(y_train)\n",
    "#     break\n",
    "# print(len(train_dataloader))\n",
    "# input_ids.shape, attention_mask.shape,token_type_ids.shape,y_train.shape\n",
    "\n",
    "# 附3  数据转化\n",
    "# 1.\n",
    "# attention_mask=[1 if i!='[PAD]' else 0 for i in tokens]\n",
    "# attention_mask\n",
    "# 2.\n",
    "# tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "# tokens_ids\n",
    "# 3.\n",
    "# numpy_info_array = np.array(info_array)  \n",
    "# print(numpy_info_array.shape)\n",
    "# 4.\n",
    "# df=pd.DataFrame(numpy_info_array,columns=col)\n",
    "# df.to_csv('./motion.csv',encoding='utf-8')\n",
    "# 5.\n",
    "# tokens_ids=torch.tensor(tokens_ids).unsqueeze(0)\n",
    "# attention_mask=torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "# 附4  使用transformers从所有的编码器层中抽取嵌入表示。\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# sentence = 'I love Paris'\n",
    "# tokens = tokenizer.tokenize(sentence)\n",
    "# tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "# tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "# attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
    "# token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "# attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "# last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask = attention_mask, return_dict = False)\n",
    "# 参数：last_hidden_state包含所有标记的嵌入表示，但是仅来自最后一个编码器层(encoder 12)\n",
    "#       pooler_output代表从最后的编码器层得到的[CLS]标记对应的嵌入表示，但进一步地通过一个线性和tanh激活函数(BertPooler)处理。\n",
    "#       hidden_states包含从所有编码器层得到的所有标记的嵌入表示\n",
    "# 1.调用模型2.切分句子3.加前后缀4.提取attention_mask5.每个标记映射到对应的id6.转化成tensor\n",
    "# class BertPooler(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "#         # to the first token.\n",
    "#         first_token_tensor = hidden_states[:, 0]\n",
    "#         # 线性\n",
    "#         pooled_output = self.dense(first_token_tensor)\n",
    "#         # tanh\n",
    "#         pooled_output = self.activation(pooled_output)\n",
    "#         return pooled_output\n",
    "# last_hidden_state.shape\n",
    "# # 它包含最后的编码器层得到的[CLS]标记对应的嵌入表示。我们打印它的形状：\n",
    "# pooler_output.shape\n",
    "# len(hidden_states)\n",
    "# hidden_states。它是一个包含13个值的元组，保存了从输入层h0到最后一个编码器层h12的所有嵌入表示：\n",
    "# 这样我们就可以得到所有编码器层的标记对应的嵌入表示\n",
    "# hidden_states[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
